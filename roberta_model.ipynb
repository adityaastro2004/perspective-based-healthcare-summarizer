{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11374992,"sourceType":"datasetVersion","datasetId":7121366}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:34:18.761825Z","iopub.execute_input":"2025-04-15T17:34:18.762073Z","iopub.status.idle":"2025-04-15T17:34:23.624335Z","shell.execute_reply.started":"2025-04-15T17:34:18.762046Z","shell.execute_reply":"2025-04-15T17:34:23.623432Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2024.12.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:34:23.625368Z","iopub.execute_input":"2025-04-15T17:34:23.625724Z","iopub.status.idle":"2025-04-15T17:34:26.655649Z","shell.execute_reply.started":"2025-04-15T17:34:23.625700Z","shell.execute_reply":"2025-04-15T17:34:26.654895Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:34:26.657769Z","iopub.execute_input":"2025-04-15T17:34:26.658258Z","iopub.status.idle":"2025-04-15T17:34:31.462412Z","shell.execute_reply.started":"2025-04-15T17:34:26.658235Z","shell.execute_reply":"2025-04-15T17:34:31.461723Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=9cfdf0f4bca8fb368737d040dac5c3dad604e5be05540ecee82ddcefd84e30f5\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:34:31.463354Z","iopub.execute_input":"2025-04-15T17:34:31.463587Z","iopub.status.idle":"2025-04-15T17:35:39.009954Z","shell.execute_reply.started":"2025-04-15T17:34:31.463564Z","shell.execute_reply":"2025-04-15T17:35:39.009170Z"}},"outputs":[{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import json\nimport argparse\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\nimport sys\nimport math\nimport warnings\nimport evaluate\nfrom torch.optim import AdamW\n\nmetric = evaluate.load(\"rouge\")\nwarnings.filterwarnings(\"ignore\")\n\n# Transformer imports\nfrom transformers import (\n    RobertaTokenizer, \n    RobertaForSequenceClassification, \n    BartTokenizer, \n    BartForConditionalGeneration,\n    AutoModelForSeq2SeqLM, \n    AutoTokenizer, \n    T5Tokenizer, \n    T5ForConditionalGeneration,\n    Seq2SeqTrainer, \n    Seq2SeqTrainingArguments, \n    DataCollatorForSeq2Seq,\n    # AdamW, \n    get_linear_schedule_with_warmup,\n    BertTokenizer,\n    BertModel\n)\nfrom transformers.file_utils import PushToHubMixin\n\n# PEFT imports\nfrom peft import (\n    get_peft_config, \n    get_peft_model, \n    get_peft_model_state_dict, \n    PrefixTuningConfig, \n    TaskType,\n    PeftModel\n)\n\n# Evaluation imports\n# from datasets import load_metric\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom scipy.spatial.distance import cosine\n# from rouge import Rouge\nfrom bert_score import score as bert_score\n\n# Set device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Define paths for Kaggle\nBASE_PATH = '/kaggle/input/nlp-project-dataset/puma_dataset'\nOUTPUT_PATH = '/kaggle/working/'\n\n# Create directories\n# os.makedirs(f\"{OUTPUT_PATH}/generated\", exist_ok=True)\n# os.makedirs(f\"{OUTPUT_PATH}/checkpoints\", exist_ok=True)\n# os.makedirs(f\"{OUTPUT_PATH}/\", exist_ok=True)\n\n# Download NLTK data\nimport nltk\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:35:39.011121Z","iopub.execute_input":"2025-04-15T17:35:39.011429Z","iopub.status.idle":"2025-04-15T17:36:05.962379Z","shell.execute_reply.started":"2025-04-15T17:35:39.011393Z","shell.execute_reply":"2025-04-15T17:36:05.961565Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 17:35:48.085506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744738548.276294      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744738548.327094      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16edf8da65144834999fc0ea2f423e83"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Classifier DataLoader Implementation\nclass ClassifierCustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.label_map = {\"EXPERIENCE\": 0, \"SUGGESTION\": 1, \"INFORMATION\": 2, \"CAUSE\": 3, \"QUESTION\": 4}\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Extract the correct input text and label based on JSON structure\n        # Assuming each example has a 'labelled_answer_spans' and 'labelled_summaries' field\n        # with keys for each perspective type\n        \n        # Get all available perspective keys\n        perspectives = list(item['labelled_summaries'].keys())\n        \n        # Select the first perspective as the label (can be modified to handle multiple perspectives)\n        if perspectives:\n            chosen_perspective = perspectives[0]\n            label = chosen_perspective\n            input_text = item['labelled_answer_spans'].get(chosen_perspective, \"\")\n        else:\n            # Fallback if no perspectives are found\n            label = \"INFORMATION\"  # Default label\n            input_text = \"\"\n            \n        # Encode label to integer\n        label_encoded = self.label_map.get(label, 0)  # Default to 0 if label not in map\n        \n        inputs = self.tokenizer(\n            input_text,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Return input_ids, attention_mask, and label\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"label\": torch.tensor(label_encoded)  # Return integer label directly\n        }\n\n# Seq2Seq DataLoader Implementation\nclass CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=1024):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n      \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Extract answers from the JSON structure\n        answers = item.get('answers', [])\n        non_empty_sentences = ' '.join([ans.replace('\\n', '') for ans in answers if ans])\n        \n        # Extract perspective information\n        # We'll select the first perspective available in labelled_summaries\n        perspectives = list(item.get('labelled_summaries', {}).keys())\n        if not perspectives:\n            # Fallback to default perspective if none found\n            perspective = \"INFORMATION\"\n            summary = \"\"\n        else:\n            perspective = perspectives[0]\n            # Use the first summary associated with this perspective\n            summary = item['labelled_summaries'][perspective]\n        \n        # Define perspective attributes based on the perspective type\n        defn = \"\"\n        start_with = \"\"\n        tone_attribute = \"\"\n        \n        if perspective == \"SUGGESTION\":\n            defn = \"Defined as advice or recommendations to assist users in making informed medical decisions, solving problems, or improving health issues.\"\n            start_with = \"It is suggested\"\n            tone_attribute = \"Advisory, Recommending\"\n            \n        elif perspective == \"INFORMATION\":\n            defn = \"Defined as knowledge about diseases, disorders, and health-related facts, providing insights into symptoms and diagnosis.\"\n            start_with = \"For information purposes\"\n            tone_attribute = \"Informative, Educational\"\n            \n        elif perspective == \"EXPERIENCE\":\n            defn = \"Defined as individual experiences, anecdotes, or firsthand insights related to health, medical treatments, medication usage, and coping strategies\"\n            start_with = \"In user's experience\"\n            tone_attribute = \"Personal, Narrative\"\n            \n        elif perspective == \"CAUSE\":\n            defn = \"Defined as reasons responsible for the occurrence of a particular medical condition, symptom, or disease\"\n            start_with = \"Some of the causes\"\n            tone_attribute = \"Explanatory, Causal\"\n            \n        elif perspective == \"QUESTION\":\n            defn = \"Defined as inquiry made for deeper understanding.\"\n            start_with = \"It is inquired\"\n            tone_attribute = \"Seeking Understanding\"\n        \n        # Check if the summary already starts with the appropriate phrase\n        if summary and len(set(summary.split(\" \")[:5]).intersection(set(start_with.split()))) >= 2:\n            target_text = summary\n        else:\n            target_text = start_with + \" \" + (summary or \"\")\n        \n        # Create the task prefix for the model\n        task_prefix = (f\"Adhering to the condition of 'begin summary with' and 'tone of summary' and \"\n                       f\"summarize according to {perspective} and start the summary with '{start_with.strip()}'. \"\n                       f\"Maintain summary tone as {tone_attribute.strip()}. Definition of perspective: {defn.strip().lower()} \"\n                       f\"Content to summarize: {non_empty_sentences} Question: {item.get('question', '').strip()}.\")\n        \n        inputs = self.tokenizer(task_prefix, padding=\"max_length\", max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n        labels = self.tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n            \n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"labels\": labels[\"input_ids\"].squeeze(),\n            \"perspective\": perspective,\n            \"Summary\": summary\n        }\n\n# DataLoader Creation Functions\ndef create_dataloader(train_dataset, valid_dataset, VALID_BATCH_SIZE, TRAIN_BATCH_SIZE):\n    train_dataloader = DataLoader(dataset=train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n    valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=True)\n    return train_dataloader, valid_dataloader\n\ndef test_create_dataloader(test_dataset, TEST_BATCH_SIZE):\n    test_dataloader = DataLoader(dataset=test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n    return test_dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:36:05.963447Z","iopub.execute_input":"2025-04-15T17:36:05.964025Z","iopub.status.idle":"2025-04-15T17:36:05.978287Z","shell.execute_reply.started":"2025-04-15T17:36:05.964005Z","shell.execute_reply":"2025-04-15T17:36:05.977615Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Validation function for classifier\ndef classifier_validation(valid_dataloader, model, VALID_BATCH_SIZE, optimizer, scheduler):\n    print(\"Validation processing...\")\n    model.eval()    \n    valid_losses = []\n  \n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(valid_dataloader)):\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n         \n            loss = output.loss\n            \n            valid_losses.append(loss.item())  \n        \n    valid_loss = np.mean(valid_losses) if len(valid_losses) > 0 else 0.0\n    return valid_loss\n\n# Function to train the classifier model\ndef train_classifier(train_data_path, valid_data_path, output_path, \n                    train_batch_size=8, valid_batch_size=4, \n                    learning_rate=1e-5, warmup_steps=4000, epochs=5):\n    \n    # Load data\n    with open(train_data_path, 'r') as json_file:\n        train_data = json.load(json_file)\n\n    with open(valid_data_path, 'r') as json_file:\n        valid_data = json.load(json_file)\n    \n    best_loss = float('inf')\n    print(\"Initial best_loss:\", best_loss)\n    last_epoch = 0\n\n    # Initialize model and tokenizer\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=5)\n            \n    # Create datasets and dataloaders\n    train_dataset = ClassifierCustomDataset(train_data, tokenizer)\n    eval_dataset = ClassifierCustomDataset(valid_data, tokenizer)\n    train_dataloader, eval_dataloader = create_dataloader(train_dataset, eval_dataset, valid_batch_size, train_batch_size)\n    \n    # Define optimizer and learning rate scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * epochs)\n\n    start_epoch = last_epoch + 1\n    num_batches = len(train_dataloader)\n\n    # Move model to device\n    model.to(device)\n    \n    # Training loop\n    for epoch in range(start_epoch, start_epoch + epochs):\n        model.train()\n        print(f\"{'#'*50} Epoch: {epoch} {'#'*50}\")\n        train_losses = []\n        \n        for i, batch in enumerate(tqdm(train_dataloader)):\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_losses.append(loss.detach())\n         \n        train_losses = [loss.item() for loss in train_losses] \n        train_loss = np.mean(train_losses)\n        print(f\"Train loss: {train_loss} for epoch: {epoch}\")\n        \n        # Validation\n        valid_loss = classifier_validation(eval_dataloader, model, valid_batch_size, optimizer, scheduler)\n        print(f\"Validation loss: {valid_loss} for epoch: {epoch}\")\n        \n        # Save best checkpoint\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            state_dict = {\n                'model_state_dict': model.state_dict(),\n                'optim_state_dict': optimizer.state_dict(),\n                'sched_state_dict': scheduler.state_dict(),\n                'loss': best_loss,\n                'epoch': epoch\n            }\n           \n            torch.save(state_dict, f\"{output_path}/best_ckpt_epoch={epoch}.ckpt\")\n            print(round(best_loss,5))\n            print(\"*\"*10 + \"Current best checkpoint is saved.\" + \"*\"*10)\n    \n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:36:05.979139Z","iopub.execute_input":"2025-04-15T17:36:05.979443Z","iopub.status.idle":"2025-04-15T17:36:07.223844Z","shell.execute_reply.started":"2025-04-15T17:36:05.979417Z","shell.execute_reply":"2025-04-15T17:36:07.223257Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class EvaluationMetrics:\n    def __init__(self, predictions, references):\n        self.predictions = predictions\n        self.references = references\n\n    def compute_rouge_score(self):\n        rouge = Rouge()\n        rouge_l_f1, rouge_l_recall, rouge_l_precision = [], [], []\n        rouge_1_f1, rouge_1_recall, rouge_1_precision = [], [], []\n        rouge_2_f1, rouge_2_recall, rouge_2_precision = [], [], []\n        for prediction, reference in zip(self.predictions, self.references):\n            scores = rouge.get_scores(prediction, reference)[0]\n            \n            rouge_l_f1.append(scores[\"rouge-l\"][\"f\"])\n            rouge_l_recall.append(scores[\"rouge-l\"][\"r\"])\n            rouge_l_precision.append(scores[\"rouge-l\"][\"p\"])\n            \n            rouge_1_f1.append(scores[\"rouge-1\"][\"f\"])\n            rouge_1_recall.append(scores[\"rouge-1\"][\"r\"])\n            rouge_1_precision.append(scores[\"rouge-1\"][\"p\"])\n            \n            rouge_2_f1.append(scores[\"rouge-2\"][\"f\"])\n            rouge_2_recall.append(scores[\"rouge-2\"][\"r\"])\n            rouge_2_precision.append(scores[\"rouge-2\"][\"p\"])\n\n        results = {\n            \"rouge_l\": {\n                \"f1\": np.mean(rouge_l_f1) * 100,\n                \"recall\": np.mean(rouge_l_recall) * 100,\n                \"precision\": np.mean(rouge_l_precision) * 100\n            },\n            \"rouge_1\": {\n                \"f1\": np.mean(rouge_1_f1) * 100,\n                \"recall\": np.mean(rouge_1_recall) * 100,\n                \"precision\": np.mean(rouge_1_precision) * 100\n            },\n            \"rouge_2\": {\n                \"f1\": np.mean(rouge_2_f1) * 100,\n                \"recall\": np.mean(rouge_2_recall) * 100,\n                \"precision\": np.mean(rouge_2_precision) * 100\n            }\n        }\n        \n        return results\n\n    def compute_meteor_score(self):\n        meteor = load_metric('meteor')\n        scores = []\n        for prediction, reference in zip(self.predictions, self.references):\n            score = meteor.compute(predictions=[prediction], references=[reference])\n            scores.append(score[\"meteor\"])\n\n        average_meteor_score = np.mean(scores)\n        \n        return {\"meteor\": average_meteor_score}\n    \n    def compute_bleu_scores(self):\n        bleu_1, bleu_2, bleu_3, bleu_4 = [], [], [], []\n        smoothie = SmoothingFunction().method4\n        for prediction, reference in zip(self.predictions, self.references):\n            reference = [reference.split()]  # BLEU score API requires tokenized sentences\n            prediction = prediction.split()\n            \n            bleu_1.append(sentence_bleu(reference, prediction, weights=(1, 0, 0, 0), smoothing_function=smoothie))\n            bleu_2.append(sentence_bleu(reference, prediction, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie))\n            bleu_3.append(sentence_bleu(reference, prediction, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie))\n            bleu_4.append(sentence_bleu(reference, prediction, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie))\n\n        results = {\n            \"bleu_1\": np.mean(bleu_1),\n            \"bleu_2\": np.mean(bleu_2),\n            \"bleu_3\": np.mean(bleu_3),\n            \"bleu_4\": np.mean(bleu_4)\n        }\n        \n        return results\n    \n    def compute_bertscore(self, lang=\"en\"):\n        P, R, F1 = bert_score(self.predictions, self.references, lang=lang)\n        return {\n            \"bertscore\": {\n                \"precision\": P.mean().item(),\n                \"recall\": R.mean().item(),\n                \"f1\": F1.mean().item()\n            }\n        }\n    \n    def evaluate_all(self):\n        \"\"\"Run all evaluation metrics and return a combined dictionary of results\"\"\"\n        results = {}\n        \n        # Get all metrics\n        rouge_results = self.compute_rouge_score()\n        meteor_results = self.compute_meteor_score()\n        bleu_results = self.compute_bleu_scores()\n        bert_results = self.compute_bertscore()\n        \n        # Combine results\n        results.update(rouge_results)\n        results.update(meteor_results)\n        results.update(bleu_results)\n        results.update(bert_results)\n        \n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:36:07.224690Z","iopub.execute_input":"2025-04-15T17:36:07.224923Z","iopub.status.idle":"2025-04-15T17:36:07.245949Z","shell.execute_reply.started":"2025-04-15T17:36:07.224892Z","shell.execute_reply":"2025-04-15T17:36:07.245326Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define paths to your data\ntrain_data_path = '/kaggle/input/nlp-project-dataset/puma_dataset/train.json'\nvalid_data_path = '/kaggle/input/nlp-project-dataset/puma_dataset/valid.json'\noutput_path = '/kaggle/working/'\n\n# Train classifier\nclassifier_model, classifier_tokenizer = train_classifier(\n    train_data_path=train_data_path,\n    valid_data_path=valid_data_path,\n    output_path=output_path,\n    train_batch_size=8,\n    valid_batch_size=4,\n    learning_rate=1e-5,\n    warmup_steps=4000,\n    epochs=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:36:07.247585Z","iopub.execute_input":"2025-04-15T17:36:07.247853Z","iopub.status.idle":"2025-04-15T17:51:05.406839Z","shell.execute_reply.started":"2025-04-15T17:36:07.247836Z","shell.execute_reply":"2025-04-15T17:51:05.406022Z"}},"outputs":[{"name":"stdout","text":"Initial best_loss: inf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab2c1eeba9943e886e51a842e8f2c6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a200fd369934dd3b2e7e136772e5a7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02cb76ec69bf4573ac6829a8d2c39927"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5ed5593fcfd4abd952a123efda1301e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed9bbbd3f5074db092f044dfaad53def"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acb2fc6f72f647ceac643d8c48cd073a"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"################################################## Epoch: 1 ##################################################\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [02:42<00:00,  1.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.184414451728974 for epoch: 1\nValidation processing...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:14<00:00, 16.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.12711860239505768 for epoch: 1\n0.12712\n**********Current best checkpoint is saved.**********\n################################################## Epoch: 2 ##################################################\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [02:41<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.08436653184305344 for epoch: 2\nValidation processing...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:14<00:00, 16.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.013780101202428341 for epoch: 2\n0.01378\n**********Current best checkpoint is saved.**********\n################################################## Epoch: 3 ##################################################\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [02:41<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.022726082552357443 for epoch: 3\nValidation processing...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:14<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.00531782815232873 for epoch: 3\n0.00532\n**********Current best checkpoint is saved.**********\n################################################## Epoch: 4 ##################################################\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [02:41<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.01168030206712761 for epoch: 4\nValidation processing...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:14<00:00, 16.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.002871677746103766 for epoch: 4\n0.00287\n**********Current best checkpoint is saved.**********\n################################################## Epoch: 5 ##################################################\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 280/280 [02:41<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.00783991087976444 for epoch: 5\nValidation processing...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 240/240 [00:14<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.0017476299544796348 for epoch: 5\n0.00175\n**********Current best checkpoint is saved.**********\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport argparse\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\nimport sys\nimport math\nimport warnings\nimport evaluate\nfrom torch.optim import AdamW\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nimport time\nfrom datetime import datetime\n\nmetric = evaluate.load(\"rouge\")\nwarnings.filterwarnings(\"ignore\")\n# Transformer imports\nfrom transformers import (\n    RobertaTokenizer, \n    RobertaForSequenceClassification, \n    BartTokenizer, \n    BartForConditionalGeneration,\n    AutoModelForSeq2SeqLM, \n    AutoTokenizer, \n    T5Tokenizer, \n    T5ForConditionalGeneration,\n    Seq2SeqTrainer, \n    Seq2SeqTrainingArguments, \n    DataCollatorForSeq2Seq,\n    get_linear_schedule_with_warmup,\n    BertTokenizer,\n    BertModel\n)\nfrom transformers.file_utils import PushToHubMixin\n# PEFT imports\nfrom peft import (\n    get_peft_config, \n    get_peft_model, \n    get_peft_model_state_dict, \n    PrefixTuningConfig, \n    TaskType,\n    PeftModel\n)\n# Evaluation imports\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom scipy.spatial.distance import cosine\nfrom bert_score import score as bert_score\n# Set device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n# Define paths for Kaggle\nBASE_PATH = '/kaggle/input/nlp-project-dataset/puma_dataset'\nOUTPUT_PATH = '/kaggle/working/'\n# Create directories\n# os.makedirs(f\"{OUTPUT_PATH}/generated\", exist_ok=True)\n# os.makedirs(f\"{OUTPUT_PATH}/checkpoints\", exist_ok=True)\n# os.makedirs(f\"{OUTPUT_PATH}/plots\", exist_ok=True)\n# Download NLTK data\nimport nltk\nnltk.download('punkt')\n\n# Classifier DataLoader Implementation\nclass ClassifierCustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.label_map = {\"EXPERIENCE\": 0, \"SUGGESTION\": 1, \"INFORMATION\": 2, \"CAUSE\": 3, \"QUESTION\": 4}\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Extract the correct input text and label based on JSON structure\n        # Assuming each example has a 'labelled_answer_spans' and 'labelled_summaries' field\n        # with keys for each perspective type\n        \n        # Get all available perspective keys\n        perspectives = list(item['labelled_summaries'].keys())\n        \n        # Select the first perspective as the label (can be modified to handle multiple perspectives)\n        if perspectives:\n            chosen_perspective = perspectives[0]\n            label = chosen_perspective\n            input_text = item['labelled_answer_spans'].get(chosen_perspective, \"\")\n        else:\n            # Fallback if no perspectives are found\n            label = \"INFORMATION\"  # Default label\n            input_text = \"\"\n            \n        # Encode label to integer\n        label_encoded = self.label_map.get(label, 0)  # Default to 0 if label not in map\n        \n        inputs = self.tokenizer(\n            input_text,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Return input_ids, attention_mask, and label\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"label\": torch.tensor(label_encoded)  # Return integer label directly\n        }\n\n# Seq2Seq DataLoader Implementation\nclass CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=1024):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n      \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Extract answers from the JSON structure\n        answers = item.get('answers', [])\n        non_empty_sentences = ' '.join([ans.replace('\\n', '') for ans in answers if ans])\n        \n        # Extract perspective information\n        # We'll select the first perspective available in labelled_summaries\n        perspectives = list(item.get('labelled_summaries', {}).keys())\n        if not perspectives:\n            # Fallback to default perspective if none found\n            perspective = \"INFORMATION\"\n            summary = \"\"\n        else:\n            perspective = perspectives[0]\n            # Use the first summary associated with this perspective\n            summary = item['labelled_summaries'][perspective]\n        \n        # Define perspective attributes based on the perspective type\n        defn = \"\"\n        start_with = \"\"\n        tone_attribute = \"\"\n        \n        if perspective == \"SUGGESTION\":\n            defn = \"Defined as advice or recommendations to assist users in making informed medical decisions, solving problems, or improving health issues.\"\n            start_with = \"It is suggested\"\n            tone_attribute = \"Advisory, Recommending\"\n            \n        elif perspective == \"INFORMATION\":\n            defn = \"Defined as knowledge about diseases, disorders, and health-related facts, providing insights into symptoms and diagnosis.\"\n            start_with = \"For information purposes\"\n            tone_attribute = \"Informative, Educational\"\n            \n        elif perspective == \"EXPERIENCE\":\n            defn = \"Defined as individual experiences, anecdotes, or firsthand insights related to health, medical treatments, medication usage, and coping strategies\"\n            start_with = \"In user's experience\"\n            tone_attribute = \"Personal, Narrative\"\n            \n        elif perspective == \"CAUSE\":\n            defn = \"Defined as reasons responsible for the occurrence of a particular medical condition, symptom, or disease\"\n            start_with = \"Some of the causes\"\n            tone_attribute = \"Explanatory, Causal\"\n            \n        elif perspective == \"QUESTION\":\n            defn = \"Defined as inquiry made for deeper understanding.\"\n            start_with = \"It is inquired\"\n            tone_attribute = \"Seeking Understanding\"\n        \n        # Check if the summary already starts with the appropriate phrase\n        if summary and len(set(summary.split(\" \")[:5]).intersection(set(start_with.split()))) >= 2:\n            target_text = summary\n        else:\n            target_text = start_with + \" \" + (summary or \"\")\n        \n        # Create the task prefix for the model\n        task_prefix = (f\"Adhering to the condition of 'begin summary with' and 'tone of summary' and \"\n                       f\"summarize according to {perspective} and start the summary with '{start_with.strip()}'. \"\n                       f\"Maintain summary tone as {tone_attribute.strip()}. Definition of perspective: {defn.strip().lower()} \"\n                       f\"Content to summarize: {non_empty_sentences} Question: {item.get('question', '').strip()}.\")\n        \n        inputs = self.tokenizer(task_prefix, padding=\"max_length\", max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n        labels = self.tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n            \n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"labels\": labels[\"input_ids\"].squeeze(),\n            \"perspective\": perspective,\n            \"Summary\": summary\n        }\n\n# DataLoader Creation Functions\ndef create_dataloader(train_dataset, valid_dataset, VALID_BATCH_SIZE, TRAIN_BATCH_SIZE):\n    train_dataloader = DataLoader(dataset=train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n    valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=True)\n    return train_dataloader, valid_dataloader\n\ndef test_create_dataloader(test_dataset, TEST_BATCH_SIZE):\n    test_dataloader = DataLoader(dataset=test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n    return test_dataloader\n\n# Validation function for classifier\ndef classifier_validation(valid_dataloader, model, VALID_BATCH_SIZE, optimizer, scheduler):\n    print(\"Validation processing...\")\n    model.eval()    \n    valid_losses = []\n    all_predictions = []\n    all_labels = []\n  \n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(valid_dataloader)):\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            \n            loss = output.loss\n            valid_losses.append(loss.item())\n            \n            # Get predictions\n            logits = output.logits\n            predictions = torch.argmax(logits, dim=-1)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n        \n    valid_loss = np.mean(valid_losses) if len(valid_losses) > 0 else 0.0\n    \n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n    \n    metrics = {\n        \"loss\": valid_loss,\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n    \n    return valid_loss, metrics, all_predictions, all_labels\n\n# Function to visualize training progress\ndef plot_training_progress(train_losses, valid_losses, metrics_history, output_path):\n    \"\"\"\n    Plot training and validation losses along with other metrics.\n    \"\"\"\n    # Convert epochs to list for x-axis\n    epochs = list(range(1, len(train_losses) + 1))\n    \n    # Plot losses\n    plt.figure(figsize=(12, 8))\n    plt.subplot(2, 2, 1)\n    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n    plt.plot(epochs, valid_losses, 'r-', label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot accuracy\n    plt.subplot(2, 2, 2)\n    plt.plot(epochs, [m['accuracy'] for m in metrics_history], 'g-')\n    plt.title('Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    \n    # Plot F1 Score\n    plt.subplot(2, 2, 3)\n    plt.plot(epochs, [m['f1'] for m in metrics_history], 'c-')\n    plt.title('Validation F1 Score')\n    plt.xlabel('Epochs')\n    plt.ylabel('F1 Score')\n    plt.grid(True)\n    \n    # Plot Precision and Recall\n    plt.subplot(2, 2, 4)\n    plt.plot(epochs, [m['precision'] for m in metrics_history], 'm-', label='Precision')\n    plt.plot(epochs, [m['recall'] for m in metrics_history], 'y-', label='Recall')\n    plt.title('Precision and Recall')\n    plt.xlabel('Epochs')\n    plt.ylabel('Score')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(f\"{output_path}/training_progress.png\")\n    plt.close()\n\n# Plot confusion matrix\ndef plot_confusion_matrix(y_true, y_pred, output_path, label_map=None):\n    \"\"\"\n    Plot confusion matrix for classifier predictions.\n    \"\"\"\n    # Create mapping from index to label name\n    if label_map:\n        index_to_label = {v: k for k, v in label_map.items()}\n        label_names = [index_to_label[i] for i in range(len(label_map))]\n    else:\n        label_names = list(range(5))  # Default to numeric labels\n    \n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n                xticklabels=label_names, \n                yticklabels=label_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.savefig(f\"{output_path}/confusion_matrix.png\")\n    plt.close()\n    \n    # Calculate and return class-wise metrics\n    report = classification_report(y_true, y_pred, target_names=label_names, output_dict=True)\n    return report\n\n# Plot learning rate\ndef plot_learning_rate(learning_rates, output_path):\n    \"\"\"\n    Plot learning rate changes over time.\n    \"\"\"\n    plt.figure(figsize=(10, 5))\n    plt.plot(learning_rates, 'b-')\n    plt.title('Learning Rate Schedule')\n    plt.xlabel('Steps')\n    plt.ylabel('Learning Rate')\n    plt.grid(True)\n    plt.savefig(f\"{output_path}/learning_rate.png\")\n    plt.close()\n\n# Evaluate model on test data and generate predictions\ndef evaluate_classifier_test(test_dataloader, model, tokenizer):\n    \"\"\"\n    Evaluate the model on test data and return predictions with analysis.\n    \"\"\"\n    model.eval()\n    all_predictions = []\n    all_input_texts = []\n    label_map = {\"EXPERIENCE\": 0, \"SUGGESTION\": 1, \"INFORMATION\": 2, \"CAUSE\": 3, \"QUESTION\": 4}\n    idx_to_label = {v: k for k, v in label_map.items()}\n    \n    with torch.no_grad():\n        for batch in tqdm(test_dataloader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            \n            # Decode input text for analysis\n            for i in range(len(input_ids)):\n                input_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n                all_input_texts.append(input_text)\n                all_predictions.append(idx_to_label[predictions[i].item()])\n    \n    results = {\n        \"texts\": all_input_texts,\n        \"predictions\": all_predictions\n    }\n    \n    return results\n\n# Sample prediction visualization\ndef visualize_sample_predictions(results, num_samples=5, output_path=None):\n    \"\"\"\n    Visualize sample predictions with text for qualitative analysis.\n    \"\"\"\n    samples = min(num_samples, len(results['texts']))\n    \n    plt.figure(figsize=(12, samples * 2))\n    \n    for i in range(samples):\n        idx = np.random.randint(0, len(results['texts']))\n        text = results['texts'][idx]\n        prediction = results['predictions'][idx]\n        \n        # Truncate text if too long\n        if len(text) > 100:\n            text = text[:97] + \"...\"\n        \n        plt.subplot(samples, 1, i+1)\n        plt.barh([prediction], [1], color='skyblue')\n        plt.title(f\"Sample {i+1}: Predicted as {prediction}\")\n        plt.yticks([prediction])\n        plt.figtext(0.5, (i+0.5)/samples - 0.05, f\"Text: {text}\", \n                   wrap=True, horizontalalignment='center', fontsize=8)\n    \n    plt.tight_layout()\n    if output_path:\n        plt.savefig(f\"{output_path}/sample_predictions.png\")\n    plt.close()\n    \n    return\n\n# Function to analyze distribution of predictions\ndef plot_prediction_distribution(predictions, output_path):\n    \"\"\"\n    Plot distribution of predicted classes.\n    \"\"\"\n    unique_labels = sorted(set(predictions))\n    counts = [predictions.count(label) for label in unique_labels]\n    \n    plt.figure(figsize=(10, 6))\n    bars = plt.bar(unique_labels, counts)\n    \n    # Add count labels on top of bars\n    for bar, count in zip(bars, counts):\n        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n                str(count), ha='center', fontweight='bold')\n    \n    plt.title('Distribution of Predicted Classes')\n    plt.xlabel('Class')\n    plt.ylabel('Count')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.savefig(f\"{output_path}/prediction_distribution.png\")\n    plt.close()\n\n# Function to train the classifier model with enhanced monitoring and visualization\ndef train_classifier(train_data_path, valid_data_path, output_path, \n                    train_batch_size=8, valid_batch_size=4, \n                    learning_rate=1e-5, warmup_steps=4000, epochs=5):\n    \n    # Load data\n    with open(train_data_path, 'r') as json_file:\n        train_data = json.load(json_file)\n    with open(valid_data_path, 'r') as json_file:\n        valid_data = json.load(json_file)\n    \n    best_loss = float('inf')\n    print(\"Initial best_loss:\", best_loss)\n    last_epoch = 0\n    # Initialize model and tokenizer\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=5)\n            \n    # Create datasets and dataloaders\n    train_dataset = ClassifierCustomDataset(train_data, tokenizer)\n    eval_dataset = ClassifierCustomDataset(valid_data, tokenizer)\n    train_dataloader, eval_dataloader = create_dataloader(train_dataset, eval_dataset, valid_batch_size, train_batch_size)\n    \n    # Define optimizer and learning rate scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * epochs)\n    start_epoch = last_epoch + 1\n    num_batches = len(train_dataloader)\n    \n    # Move model to device\n    model.to(device)\n    \n    # Initialize tracking metrics\n    train_losses_history = []\n    valid_losses_history = []\n    metrics_history = []\n    learning_rates = []\n    \n    start_time = time.time()\n    \n    # Training loop\n    for epoch in range(start_epoch, start_epoch + epochs):\n        model.train()\n        print(f\"{'#'*50} Epoch: {epoch} {'#'*50}\")\n        train_losses = []\n        \n        # Training\n        for i, batch in enumerate(tqdm(train_dataloader)):\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            \n            # Track learning rate\n            learning_rates.append(scheduler.get_last_lr()[0])\n            \n            scheduler.step()\n            train_losses.append(loss.detach().item())\n            \n            # Log progress periodically\n            if i % 50 == 0:\n                print(f\"Epoch {epoch}, Batch {i}/{num_batches}, Loss: {loss.item():.4f}\")\n         \n        avg_train_loss = np.mean(train_losses)\n        train_losses_history.append(avg_train_loss)\n        print(f\"Train loss: {avg_train_loss} for epoch: {epoch}\")\n        \n        # Validation\n        valid_loss, metrics, all_predictions, all_labels = classifier_validation(\n            eval_dataloader, model, valid_batch_size, optimizer, scheduler\n        )\n        valid_losses_history.append(valid_loss)\n        metrics_history.append(metrics)\n        \n        print(f\"Validation metrics for epoch {epoch}:\")\n        for k, v in metrics.items():\n            print(f\"  {k}: {v:.4f}\")\n        \n        # Save best checkpoint\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            state_dict = {\n                'model_state_dict': model.state_dict(),\n                'optim_state_dict': optimizer.state_dict(),\n                'sched_state_dict': scheduler.state_dict(),\n                'loss': best_loss,\n                'epoch': epoch,\n                'metrics': metrics\n            }\n           \n            checkpoint_path = f\"{output_path}/best_ckpt_epoch={epoch}.ckpt\"\n            torch.save(state_dict, checkpoint_path)\n            print(f\"Best loss: {round(best_loss,5)}\")\n            print(\"*\"*10 + \"Current best checkpoint is saved at \" + checkpoint_path + \"*\"*10)\n            \n            # Plot confusion matrix for best model\n            report = plot_confusion_matrix(all_labels, all_predictions, output_path, \n                                          label_map=train_dataset.label_map)\n            \n            # Save classification report\n            with open(f\"{output_path}/best_classification_report.json\", 'w') as f:\n                json.dump(report, f, indent=4)\n    \n    # Calculate training time\n    training_time = time.time() - start_time\n    print(f\"Total training time: {training_time/60:.2f} minutes\")\n    \n    # Plot training progress\n    plot_training_progress(train_losses_history, valid_losses_history, metrics_history, output_path)\n    \n    # Plot learning rate\n    plot_learning_rate(learning_rates, output_path)\n    \n    # Save training history for future reference\n    history = {\n        'train_losses': train_losses_history,\n        'valid_losses': valid_losses_history,\n        'metrics': metrics_history,\n        'training_time': training_time,\n        'best_epoch': epoch,\n        'best_loss': best_loss\n    }\n    \n    with open(f\"{output_path}/training_history.json\", 'w') as f:\n        json.dump(history, f, indent=4)\n    \n    return model, tokenizer\n\n# Function to generate predictions from summarization model\ndef generate_summaries(model, tokenizer, test_data, output_path, batch_size=4, num_beams=4, max_length=150):\n    \"\"\"\n    Generate summaries from the trained summarization model.\n    \"\"\"\n    model.eval()\n    test_dataset = CustomDataset(test_data, tokenizer)\n    test_dataloader = test_create_dataloader(test_dataset, batch_size)\n    \n    all_generated_summaries = []\n    all_reference_summaries = []\n    all_perspectives = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_dataloader):\n            # Move batch to device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            # Generate summaries\n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True\n            )\n            \n            # Decode the generated summaries\n            generated_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            \n            # Get reference summaries and perspectives\n            reference_summaries = [tokenizer.decode(batch['labels'][i], skip_special_tokens=True) \n                                 for i in range(len(batch['labels']))]\n            perspectives = batch['perspective']\n            \n            # Append to lists\n            all_generated_summaries.extend(generated_summaries)\n            all_reference_summaries.extend(reference_summaries)\n            all_perspectives.extend(perspectives)\n    \n    # Calculate ROUGE scores\n    rouge_results = metric.compute(predictions=all_generated_summaries, references=all_reference_summaries)\n    \n    # Calculate BLEU scores\n    smoothie = SmoothingFunction().method1\n    bleu_scores = []\n    for gen, ref in zip(all_generated_summaries, all_reference_summaries):\n        gen_tokens = nltk.word_tokenize(gen.lower())\n        ref_tokens = [nltk.word_tokenize(ref.lower())]\n        bleu_score = sentence_bleu(ref_tokens, gen_tokens, smoothing_function=smoothie)\n        bleu_scores.append(bleu_score)\n    \n    # Save results\n    results = {\n        'generated': all_generated_summaries,\n        'reference': all_reference_summaries,\n        'perspective': all_perspectives,\n        'rouge': rouge_results,\n        'bleu': np.mean(bleu_scores)\n    }\n    \n    with open(f\"{output_path}/summary_results.json\", 'w') as f:\n        json.dump(results, f, indent=4)\n    \n    # Plot ROUGE scores by perspective\n    perspectives_set = list(set(all_perspectives))\n    rouge_by_perspective = {p: {'rouge1': [], 'rouge2': [], 'rougeL': []} for p in perspectives_set}\n    \n    # Group ROUGE scores by perspective\n    for i, p in enumerate(all_perspectives):\n        rouge1 = metric.compute(predictions=[all_generated_summaries[i]], \n                              references=[all_reference_summaries[i]])['rouge1']\n        rouge2 = metric.compute(predictions=[all_generated_summaries[i]], \n                              references=[all_reference_summaries[i]])['rouge2']\n        rougeL = metric.compute(predictions=[all_generated_summaries[i]], \n                              references=[all_reference_summaries[i]])['rougeL']\n        \n        rouge_by_perspective[p]['rouge1'].append(rouge1)\n        rouge_by_perspective[p]['rouge2'].append(rouge2)\n        rouge_by_perspective[p]['rougeL'].append(rougeL)\n    \n    # Calculate mean ROUGE scores by perspective\n    rouge_means = {p: {k: np.mean(v) for k, v in scores.items()} \n                 for p, scores in rouge_by_perspective.items()}\n    \n    # Plot\n    plt.figure(figsize=(12, 6))\n    x = np.arange(len(perspectives_set))\n    width = 0.25\n    \n    plt.bar(x - width, [rouge_means[p]['rouge1'] for p in perspectives_set], width, label='ROUGE-1')\n    plt.bar(x, [rouge_means[p]['rouge2'] for p in perspectives_set], width, label='ROUGE-2')\n    plt.bar(x + width, [rouge_means[p]['rougeL'] for p in perspectives_set], width, label='ROUGE-L')\n    \n    plt.xlabel('Perspective')\n    plt.ylabel('Score')\n    plt.title('ROUGE Scores by Perspective')\n    plt.xticks(x, perspectives_set)\n    plt.legend()\n    plt.savefig(f\"{output_path}/rouge_by_perspective.png\")\n    plt.close()\n    \n    return results\n\n# Function to train summarization model\ndef train_seq2seq_model(train_data_path, valid_data_path, output_path, \n                        model_name=\"t5-base\", train_batch_size=4, valid_batch_size=4,\n                        learning_rate=5e-5, num_epochs=3):\n    \"\"\"\n    Train a sequence-to-sequence model for text summarization.\n    \"\"\"\n    # Load data\n    with open(train_data_path, 'r') as json_file:\n        train_data = json.load(json_file)\n    with open(valid_data_path, 'r') as json_file:\n        valid_data = json.load(json_file)\n    \n    # Initialize tokenizer and model\n    tokenizer = T5Tokenizer.from_pretrained(model_name)\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    \n    # Create datasets\n    train_dataset = CustomDataset(train_data, tokenizer)\n    valid_dataset = CustomDataset(valid_data, tokenizer)\n    \n    # Data collator\n    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n    \n    # Define training arguments\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=f\"{output_path}\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=train_batch_size,\n        per_device_eval_batch_size=valid_batch_size,\n        weight_decay=0.01,\n        save_total_limit=3,\n        num_train_epochs=num_epochs,\n        predict_with_generate=True,\n        save_strategy=\"epoch\",\n        logging_dir=f\"{output_path}/logs\",\n        logging_steps=100,\n    )\n    \n    # Initialize Trainer\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    # Train model\n    start_time = time.time()\n    print(\"Starting training...\")\n    trainer.train()\n    training_time = time.time() - start_time\n    print(f\"Training completed in {training_time/60:.2f} minutes\")\n    \n    # Save the final model\n    trainer.save_model(f\"{output_path}/final_model\")\n    \n    return model, tokenizer\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:02:49.614533Z","iopub.execute_input":"2025-04-15T18:02:49.615170Z","iopub.status.idle":"2025-04-15T18:02:50.119173Z","shell.execute_reply.started":"2025-04-15T18:02:49.615147Z","shell.execute_reply":"2025-04-15T18:02:50.118335Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# # Main function to run the entire pipeline\n# def main():\n#     \"\"\"\n#     Run the entire pipeline: training, evaluation, and visualization.\n#     \"\"\"\n#     parser = argparse.ArgumentParser(description='Train and evaluate NLP models for perspective classification and summarization')\n    \n#     # Add arguments\n#     parser.add_argument('--train_file', type=str, default=f'{BASE_PATH}/train.json', \n#                         help='Path to training data file')\n#     parser.add_argument('--valid_file', type=str, default=f'{BASE_PATH}/valid.json', \n#                         help='Path to validation data file')\n#     parser.add_argument('--test_file', type=str, default=f'{BASE_PATH}/test.json', \n#                         help='Path to test data file')\n#     parser.add_argument('--output_dir', type=str, default=OUTPUT_PATH, \n#                         help='Directory to save outputs')\n#     parser.add_argument('--task', type=str, choices=['classifier', 'summarizer', 'all'], default='all',\n#                         help='Task to run: classifier, summarizer, or all')\n#     parser.add_argument('--classifier_epochs', type=int, default=5, \n#                         help='Number of epochs for classifier training')\n#     parser.add_argument('--summarizer_epochs', type=int, default=3, \n#                         help='Number of epochs for summarizer training')\n#     parser.add_argument('--classifier_batch_size', type=int, default=8, \n#                         help='Batch size for classifier training')\n#     parser.add_argument('--summarizer_batch_size', type=int, default=4, \n#                         help='Batch size for summarizer training')\n#     parser.add_argument('--classifier_lr', type=float, default=1e-5, \n#                         help='Learning rate for classifier')\n#     parser.add_argument('--summarizer_lr', type=float, default=5e-5, \n#                         help='Learning rate for summarizer')\n#     parser.add_argument('--summarizer_model', type=str, default='t5-base', \n#                         help='Model name for summarizer')\n    \n#     args = parser.parse_args()\n    \n#     # Set up logging\n#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n#     log_file = f\"{args.output_dir}/run_log_{timestamp}.txt\"\n    \n#     # Print and log basic info\n#     print(f\"Starting experiment at {timestamp}\")\n#     print(f\"Using device: {device}\")\n#     print(f\"Args: {args}\")\n    \n#     # Training and evaluation\n#     if args.task in ['classifier', 'all']:\n#         print(\"\\n\" + \"=\"*50)\n#         print(\"TRAINING CLASSIFIER MODEL\")\n#         print(\"=\"*50)\n        \n#         # Train classifier\n#         classifier_model, classifier_tokenizer = train_classifier(\n#             train_data_path=args.train_file,\n#             valid_data_path=args.valid_file,\n#             output_path=args.output_dir,\n#             train_batch_size=args.classifier_batch_size,\n#             valid_batch_size=args.classifier_batch_size // 2,\n#             learning_rate=args.classifier_lr,\n#             warmup_steps=100,\n#             epochs=args.classifier_epochs\n#         )\n        \n#         # Load test data for classifier evaluation\n#         with open(args.test_file, 'r') as f:\n#             test_data = json.load(f)\n        \n#         # Create test dataset and dataloader\n#         test_dataset = ClassifierCustomDataset(test_data, classifier_tokenizer)\n#         test_dataloader = test_create_dataloader(test_dataset, args.classifier_batch_size)\n        \n#         # Evaluate on test data\n#         print(\"\\nEvaluating classifier on test data...\")\n#         test_results = evaluate_classifier_test(test_dataloader, classifier_model, classifier_tokenizer)\n        \n#         # Visualize sample predictions\n#         visualize_sample_predictions(test_results, num_samples=5, output_path=args.output_dir)\n        \n#         # Plot distribution of predictions\n#         plot_prediction_distribution(test_results['predictions'], args.output_dir)\n        \n#         print(\"\\nClassifier training and evaluation completed!\")\n    \n#     if args.task in ['summarizer', 'all']:\n#         print(\"\\n\" + \"=\"*50)\n#         print(\"TRAINING SUMMARIZATION MODEL\")\n#         print(\"=\"*50)\n        \n#         # Train summarization model\n#         summarizer_model, summarizer_tokenizer = train_seq2seq_model(\n#             train_data_path=args.train_file,\n#             valid_data_path=args.valid_file,\n#             output_path=args.output_dir,\n#             model_name=args.summarizer_model,\n#             train_batch_size=args.summarizer_batch_size,\n#             valid_batch_size=args.summarizer_batch_size // 2,\n#             learning_rate=args.summarizer_lr,\n#             num_epochs=args.summarizer_epochs\n#         )\n        \n#         # Load test data for summarizer evaluation\n#         with open(args.test_file, 'r') as f:\n#             test_data = json.load(f)\n        \n#         # Generate summaries and evaluate\n#         print(\"\\nGenerating summaries on test data...\")\n#         summary_results = generate_summaries(\n#             model=summarizer_model,\n#             tokenizer=summarizer_tokenizer,\n#             test_data=test_data,\n#             output_path=args.output_dir,\n#             batch_size=args.summarizer_batch_size,\n#             num_beams=4,\n#             max_length=150\n#         )\n        \n#         # Print summary evaluation results\n#         print(f\"\\nSummarization Results:\")\n#         print(f\"ROUGE-1: {summary_results['rouge']['rouge1']:.4f}\")\n#         print(f\"ROUGE-2: {summary_results['rouge']['rouge2']:.4f}\")\n#         print(f\"ROUGE-L: {summary_results['rouge']['rougeL']:.4f}\")\n#         print(f\"BLEU: {summary_results['bleu']:.4f}\")\n        \n#         print(\"\\nSummarizer training and evaluation completed!\")\n    \n#     # Generate combined report\n#     if args.task == 'all':\n#         generate_combined_report(args.output_dir)\n    \n#     print(\"\\n\" + \"=\"*50)\n#     print(f\"EXPERIMENT COMPLETED! Results saved to {args.output_dir}\")\n#     print(\"=\"*50)\n\n# def generate_combined_report(output_path):\n#     \"\"\"\n#     Generate a combined report of classifier and summarizer results.\n#     \"\"\"\n#     # Create a comprehensive report figure\n#     plt.figure(figsize=(15, 12))\n    \n#     # Add classifier confusion matrix\n#     try:\n#         cm_img = plt.imread(f\"{output_path}/plots/confusion_matrix.png\")\n#         plt.subplot(2, 2, 1)\n#         plt.imshow(cm_img)\n#         plt.axis('off')\n#         plt.title(\"Classifier Confusion Matrix\")\n#     except:\n#         print(\"Could not find confusion matrix image\")\n    \n#     # Add training progress\n#     try:\n#         prog_img = plt.imread(f\"{output_path}/plots/training_progress.png\")\n#         plt.subplot(2, 2, 2)\n#         plt.imshow(prog_img)\n#         plt.axis('off')\n#         plt.title(\"Training Progress\")\n#     except:\n#         print(\"Could not find training progress image\")\n    \n#     # Add summarizer ROUGE scores\n#     try:\n#         rouge_img = plt.imread(f\"{output_path}/plots/rouge_by_perspective.png\")\n#         plt.subplot(2, 2, 3)\n#         plt.imshow(rouge_img)\n#         plt.axis('off')\n#         plt.title(\"Summarizer ROUGE Scores by Perspective\")\n#     except:\n#         print(\"Could not find ROUGE scores image\")\n    \n#     # Add prediction distribution\n#     try:\n#         dist_img = plt.imread(f\"{output_path}/plots/prediction_distribution.png\")\n#         plt.subplot(2, 2, 4)\n#         plt.imshow(dist_img)\n#         plt.axis('off')\n#         plt.title(\"Prediction Distribution\")\n#     except:\n#         print(\"Could not find prediction distribution image\")\n    \n#     plt.tight_layout()\n#     plt.savefig(f\"{output_path}/combined_report.png\", dpi=300)\n#     plt.close()\n    \n#     # Gather metrics from files\n#     metrics = {}\n#     try:\n#         with open(f\"{output_path}/training_history.json\", 'r') as f:\n#             training_history = json.load(f)\n#         metrics['training'] = training_history\n#     except:\n#         print(\"Could not load training history\")\n    \n#     try:\n#         with open(f\"{output_path}/best_classification_report.json\", 'r') as f:\n#             class_report = json.load(f)\n#         metrics['classification'] = class_report\n#     except:\n#         print(\"Could not load classification report\")\n    \n#     try:\n#         with open(f\"{output_path}/generated/summary_results.json\", 'r') as f:\n#             summary_results = json.load(f)\n#         metrics['summarization'] = {\n#             'rouge': summary_results['rouge'],\n#             'bleu': summary_results['bleu']\n#         }\n#     except:\n#         print(\"Could not load summarization results\")\n    \n#     # Save combined metrics\n#     with open(f\"{output_path}/combined_metrics.json\", 'w') as f:\n#         json.dump(metrics, f, indent=4)\n    \n#     # Generate simple text report\n#     with open(f\"{output_path}/final_report.txt\", 'w') as f:\n#         f.write(\"=\" * 50 + \"\\n\")\n#         f.write(\"COMBINED RESULTS REPORT\\n\")\n#         f.write(\"=\" * 50 + \"\\n\\n\")\n        \n#         # Classifier results\n#         f.write(\"CLASSIFIER RESULTS:\\n\")\n#         f.write(\"-\" * 30 + \"\\n\")\n#         if 'classification' in metrics:\n#             f.write(f\"Accuracy: {metrics['classification']['accuracy']:.4f}\\n\")\n#             f.write(\"Class-wise Performance:\\n\")\n#             for cls in metrics['classification']:\n#                 if cls not in ['accuracy', 'macro avg', 'weighted avg']:\n#                     f.write(f\"  {cls}: F1={metrics['classification'][cls]['f1']:.4f}, \"\n#                            f\"Precision={metrics['classification'][cls]['precision']:.4f}, \"\n#                            f\"Recall={metrics['classification'][cls]['recall']:.4f}\\n\")\n#         else:\n#             f.write(\"No classification metrics available\\n\")\n        \n#         f.write(\"\\n\")\n        \n#         # Summarizer results\n#         f.write(\"SUMMARIZER RESULTS:\\n\")\n#         f.write(\"-\" * 30 + \"\\n\")\n#         if 'summarization' in metrics:\n#             f.write(f\"ROUGE-1: {metrics['summarization']['rouge']['rouge1']:.4f}\\n\")\n#             f.write(f\"ROUGE-2: {metrics['summarization']['rouge']['rouge2']:.4f}\\n\")\n#             f.write(f\"ROUGE-L: {metrics['summarization']['rouge']['rougeL']:.4f}\\n\")\n#             f.write(f\"BLEU: {metrics['summarization']['bleu']:.4f}\\n\")\n#         else:\n#             f.write(\"No summarization metrics available\\n\")\n        \n#         f.write(\"\\n\")\n#         f.write(\"=\" * 50 + \"\\n\")\n#         f.write(\"END OF REPORT\\n\")\n#         f.write(\"=\" * 50 + \"\\n\")\n\n# # Run the main function if script is executed directly\n# if __name__ == \"__main__\":\n#     main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:51:20.998464Z","iopub.execute_input":"2025-04-15T17:51:20.999347Z","iopub.status.idle":"2025-04-15T17:51:21.026012Z","shell.execute_reply.started":"2025-04-15T17:51:20.999321Z","shell.execute_reply":"2025-04-15T17:51:21.025253Z"}},"outputs":[{"name":"stderr","text":"usage: colab_kernel_launcher.py [-h] [--train_file TRAIN_FILE] [--valid_file VALID_FILE]\n                                [--test_file TEST_FILE] [--output_dir OUTPUT_DIR]\n                                [--task {classifier,summarizer,all}]\n                                [--classifier_epochs CLASSIFIER_EPOCHS]\n                                [--summarizer_epochs SUMMARIZER_EPOCHS]\n                                [--classifier_batch_size CLASSIFIER_BATCH_SIZE]\n                                [--summarizer_batch_size SUMMARIZER_BATCH_SIZE]\n                                [--classifier_lr CLASSIFIER_LR] [--summarizer_lr SUMMARIZER_LR]\n                                [--summarizer_model SUMMARIZER_MODEL]\ncolab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-935c0797-664f-40b0-aba1-ecb0bf1b7e3c.json\n","output_type":"stream"},{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"],"ename":"SystemExit","evalue":"2","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"# Main function adapted for Jupyter Notebook\ndef main(train_file=f'{BASE_PATH}/train.json',\n         valid_file=f'{BASE_PATH}/valid.json',\n         test_file=f'{BASE_PATH}/test.json',\n         output_dir=OUTPUT_PATH,\n         task='all',\n         classifier_epochs=5,\n         summarizer_epochs=3,\n         classifier_batch_size=8,\n         summarizer_batch_size=4,\n         classifier_lr=1e-5,\n         summarizer_lr=5e-5,\n         summarizer_model='t5-base'):\n    \"\"\"\n    Run the entire pipeline: training, evaluation, and visualization.\n    All parameters are optional with defaults.\n    \n    Parameters:\n    -----------\n    train_file : str\n        Path to training data file\n    valid_file : str\n        Path to validation data file\n    test_file : str\n        Path to test data file\n    output_dir : str\n        Directory to save outputs\n    task : str\n        Task to run: 'classifier', 'summarizer', or 'all'\n    classifier_epochs : int\n        Number of epochs for classifier training\n    summarizer_epochs : int\n        Number of epochs for summarizer training\n    classifier_batch_size : int\n        Batch size for classifier training\n    summarizer_batch_size : int\n        Batch size for summarizer training\n    classifier_lr : float\n        Learning rate for classifier\n    summarizer_lr : float\n        Learning rate for summarizer\n    summarizer_model : str\n        Model name for summarizer\n    \"\"\"\n    # Set up logging\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = f\"{output_dir}/run_log_{timestamp}.txt\"\n    \n    # Print basic info\n    print(f\"Starting experiment at {timestamp}\")\n    print(f\"Using device: {device}\")\n    \n    # Training and evaluation\n    if task in ['classifier', 'all']:\n        print(\"\\n\" + \"=\"*50)\n        print(\"TRAINING CLASSIFIER MODEL\")\n        print(\"=\"*50)\n        \n        # Train classifier\n        classifier_model, classifier_tokenizer = train_classifier(\n            train_data_path=train_file,\n            valid_data_path=valid_file,\n            output_path=output_dir,\n            train_batch_size=classifier_batch_size,\n            valid_batch_size=classifier_batch_size // 2,\n            learning_rate=classifier_lr,\n            warmup_steps=100,\n            epochs=classifier_epochs\n        )\n        \n        # Load test data for classifier evaluation\n        with open(test_file, 'r') as f:\n            test_data = json.load(f)\n        \n        # Create test dataset and dataloader\n        test_dataset = ClassifierCustomDataset(test_data, classifier_tokenizer)\n        test_dataloader = test_create_dataloader(test_dataset, classifier_batch_size)\n        \n        # Evaluate on test data\n        print(\"\\nEvaluating classifier on test data...\")\n        test_results = evaluate_classifier_test(test_dataloader, classifier_model, classifier_tokenizer)\n        \n        # Visualize sample predictions\n        visualize_sample_predictions(test_results, num_samples=5, output_path=output_dir)\n        \n        # Plot distribution of predictions\n        plot_prediction_distribution(test_results['predictions'], output_dir)\n        \n        print(\"\\nClassifier training and evaluation completed!\")\n    \n    if task in ['summarizer', 'all']:\n        print(\"\\n\" + \"=\"*50)\n        print(\"TRAINING SUMMARIZATION MODEL\")\n        print(\"=\"*50)\n        \n        # Train summarization model\n        summarizer_model, summarizer_tokenizer = train_seq2seq_model(\n            train_data_path=train_file,\n            valid_data_path=valid_file,\n            output_path=output_dir,\n            model_name=summarizer_model,\n            train_batch_size=summarizer_batch_size,\n            valid_batch_size=summarizer_batch_size // 2,\n            learning_rate=summarizer_lr,\n            num_epochs=summarizer_epochs\n        )\n        \n        # Load test data for summarizer evaluation\n        with open(test_file, 'r') as f:\n            test_data = json.load(f)\n        \n        # Generate summaries and evaluate\n        print(\"\\nGenerating summaries on test data...\")\n        summary_results = generate_summaries(\n            model=summarizer_model,\n            tokenizer=summarizer_tokenizer,\n            test_data=test_data,\n            output_path=output_dir,\n            batch_size=summarizer_batch_size,\n            num_beams=4,\n            max_length=150\n        )\n        \n        # Print summary evaluation results\n        print(f\"\\nSummarization Results:\")\n        print(f\"ROUGE-1: {summary_results['rouge']['rouge1']:.4f}\")\n        print(f\"ROUGE-2: {summary_results['rouge']['rouge2']:.4f}\")\n        print(f\"ROUGE-L: {summary_results['rouge']['rougeL']:.4f}\")\n        print(f\"BLEU: {summary_results['bleu']:.4f}\")\n        \n        print(\"\\nSummarizer training and evaluation completed!\")\n    \n    # Generate combined report\n    if task == 'all':\n        generate_combined_report(output_dir)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(f\"EXPERIMENT COMPLETED! Results saved to {output_dir}\")\n    print(\"=\"*50)\n    \n    return {\n        'classifier_model': classifier_model if task in ['classifier', 'all'] else None,\n        'classifier_tokenizer': classifier_tokenizer if task in ['classifier', 'all'] else None,\n        'summarizer_model': summarizer_model if task in ['summarizer', 'all'] else None,\n        'summarizer_tokenizer': summarizer_tokenizer if task in ['summarizer', 'all'] else None\n    }\n\n# Example usage in Jupyter Notebook:\nmodels = main(\n    task='all',                      # 'classifier', 'summarizer', or 'all'\n    classifier_epochs=3,             # Reduce for faster execution\n    summarizer_epochs=2,             # Reduce for faster execution\n    classifier_batch_size=32,         # Adjust based on your GPU memory\n    summarizer_batch_size=16          # Adjust based on your GPU memory\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:02:55.492157Z","iopub.execute_input":"2025-04-15T18:02:55.492875Z","iopub.status.idle":"2025-04-15T18:05:42.794452Z","shell.execute_reply.started":"2025-04-15T18:02:55.492854Z","shell.execute_reply":"2025-04-15T18:05:42.792882Z"}},"outputs":[{"name":"stdout","text":"Starting experiment at 20250415_180255\nUsing device: cuda\n\n==================================================\nTRAINING CLASSIFIER MODEL\n==================================================\nInitial best_loss: inf\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"################################################## Epoch: 1 ##################################################\n","output_type":"stream"},{"name":"stderr","text":"  1%|▏         | 1/70 [00:02<02:28,  2.15s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Batch 0/70, Loss: 1.6334\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 51/70 [01:47<00:40,  2.12s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Batch 50/70, Loss: 0.2559\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [02:27<00:00,  2.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9678604668272394 for epoch: 1\nValidation processing...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 60/60 [00:14<00:00,  4.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation metrics for epoch 1:\n  loss: 0.0265\n  accuracy: 1.0000\n  precision: 1.0000\n  recall: 1.0000\n  f1: 1.0000\nBest loss: 0.02647\n**********Current best checkpoint is saved at /kaggle/working//best_ckpt_epoch=1.ckpt**********\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2476918775.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# Example usage in Jupyter Notebook:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m models = main(\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0;31m# 'classifier', 'summarizer', or 'all'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mclassifier_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;31m# Reduce for faster execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2476918775.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_file, valid_file, test_file, output_dir, task, classifier_epochs, summarizer_epochs, classifier_batch_size, summarizer_batch_size, classifier_lr, summarizer_lr, summarizer_model)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Train classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         classifier_model, classifier_tokenizer = train_classifier(\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mtrain_data_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mvalid_data_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1399627084.py\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(train_data_path, valid_data_path, output_path, train_batch_size, valid_batch_size, learning_rate, warmup_steps, epochs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# Plot confusion matrix for best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             report = plot_confusion_matrix(all_labels, all_predictions, output_path, \n\u001b[0m\u001b[1;32m    531\u001b[0m                                           label_map=train_dataset.label_map)\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1399627084.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(y_true, y_pred, output_path, label_map)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;31m# Calculate and return class-wise metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2330\u001b[0m             )\n\u001b[1;32m   2331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2333\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Number of classes, 1, does not match size of target_names, 5. Try specifying the labels parameter"],"ename":"ValueError","evalue":"Number of classes, 1, does not match size of target_names, 5. Try specifying the labels parameter","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"# Interactive widgets implementation\n# First make sure to install ipywidgets if not already installed\n# !pip install ipywidgets\n\nimport ipywidgets as widgets\nfrom IPython.display import display\n\ndef run_with_widgets():\n    # Create widgets for each parameter\n    task_widget = widgets.RadioButtons(\n        options=['classifier', 'summarizer', 'all'],\n        value='all',\n        description='Task:',\n        disabled=False\n    )\n    \n    classifier_epochs_widget = widgets.IntSlider(\n        value=3,\n        min=1,\n        max=10,\n        step=1,\n        description='Classifier Epochs:',\n        disabled=False,\n        continuous_update=False\n    )\n    \n    summarizer_epochs_widget = widgets.IntSlider(\n        value=2,\n        min=1,\n        max=5,\n        step=1,\n        description='Summarizer Epochs:',\n        disabled=False,\n        continuous_update=False\n    )\n    \n    classifier_batch_size_widget = widgets.IntSlider(\n        value=8,\n        min=2,\n        max=16,\n        step=2,\n        description='Classifier Batch Size:',\n        disabled=False,\n        continuous_update=False\n    )\n    \n    summarizer_batch_size_widget = widgets.IntSlider(\n        value=4,\n        min=2,\n        max=8,\n        step=2,\n        description='Summarizer Batch Size:',\n        disabled=False,\n        continuous_update=False\n    )\n    \n    classifier_lr_widget = widgets.FloatLogSlider(\n        value=1e-5,\n        base=10,\n        min=-6,  # 10^-6\n        max=-4,  # 10^-4\n        step=0.1,\n        description='Classifier LR:',\n        disabled=False,\n        continuous_update=False\n    )\n    \n    summarizer_lr_widget = widgets.FloatLogSlider(\n        value=5e-5,\n        base=10,\n        min=-6,  # 10^-6\n        max=-4,  # 10^-4\n        step=0.1,\n        description='Summarizer LR:',\n        disabled=False,\n        continuous_update=False\n    )\n    \n    model_widget = widgets.Dropdown(\n        options=['t5-small', 't5-base', 't5-large'],\n        value='t5-base',\n        description='Summarizer Model:',\n        disabled=False\n    )\n    \n    output_widget = widgets.Output()\n    \n    # Button to start training\n    button = widgets.Button(\n        description='Start Training',\n        disabled=False,\n        button_style='success',\n        tooltip='Click to start the training process',\n        icon='play'\n    )\n    \n    # Function to run when button is clicked\n    def on_button_clicked(b):\n        with output_widget:\n            # Clear previous outputs\n            output_widget.clear_output()\n            \n            # Run the main function with the selected parameters\n            print(\"Starting training with selected parameters...\")\n            models = main(\n                task=task_widget.value,\n                classifier_epochs=classifier_epochs_widget.value,\n                summarizer_epochs=summarizer_epochs_widget.value,\n                classifier_batch_size=classifier_batch_size_widget.value,\n                summarizer_batch_size=summarizer_batch_size_widget.value,\n                classifier_lr=classifier_lr_widget.value,\n                summarizer_lr=summarizer_lr_widget.value,\n                summarizer_model=model_widget.value\n            )\n            print(\"Training complete!\")\n            \n    # Attach the function to the button\n    button.on_click(on_button_clicked)\n    \n    # Create layout for the widgets\n    left_box = widgets.VBox([task_widget, classifier_epochs_widget, summarizer_epochs_widget, \n                            classifier_batch_size_widget, summarizer_batch_size_widget])\n    right_box = widgets.VBox([classifier_lr_widget, summarizer_lr_widget, model_widget, button])\n    controls = widgets.HBox([left_box, right_box])\n    \n    # Display the widgets and output\n    display(controls)\n    display(output_widget)\n\n# Run this cell to display the interactive interface\nrun_with_widgets()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}